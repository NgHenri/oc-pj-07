{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOhcf8Qzh4fZa5lVDccGBWp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bYWcfqWd8ISZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b910aa91-e631-4783-c83d-98bf48c75c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Tu es sur Google Colab\n",
            "Chemin d'accès aux données: /content/drive/MyDrive/Python/OCRP/Projet07/data/credit-default-risk\n"
          ]
        }
      ],
      "source": [
        "# Standard Libraries\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "import unicodedata\n",
        "\n",
        "# Data Manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from os import listdir\n",
        "from joblib import dump\n",
        "from joblib import load\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "# Détermine si vous êtes sur Google Colab\n",
        "is_colab = 'google.colab' in sys.modules\n",
        "\n",
        "# Détermine si vous êtes sur Kaggle\n",
        "is_kaggle = '/kaggle' in os.getcwd()\n",
        "\n",
        "# Chemin par défaut\n",
        "path = None\n",
        "\n",
        "if is_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    path = '/content/drive/MyDrive/Python/OCRP/Projet07/data/credit-default-risk'\n",
        "    output_dir = '/content/drive/MyDrive/Python/OCRP/Projet07/working/graphs'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(\"\\nTu es sur Google Colab\")\n",
        "elif is_kaggle:\n",
        "    path = \"/kaggle/input/credit-risk\"\n",
        "    output_dir = '/kaggle/working/graphs'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(\"\\nTu es sur Kaggle\")\n",
        "else:\n",
        "    path = \"~/Documents/Python/OCR/Projet07/data/credit-risk\"\n",
        "    output_dir = \"~/Documents/Python/OCR/Projet07/working/graphs\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(\"\\nTu es en local\")\n",
        "\n",
        "# Utilisez le chemin d'accès sélectionné pour accéder à vos données\n",
        "if path is not None:\n",
        "    print(\"Chemin d'accès aux données:\", path)\n",
        "else:\n",
        "    print(\"Impossible de déterminer l'environnement.\")\n",
        "# Chemin d'accès aux données\n",
        "save_path = \"/kaggle/working/\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des données"
      ],
      "metadata": {
        "id": "lWYsIzo8AroG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le DataFrame avec pandas\n",
        "path = '/content/drive/MyDrive/Python/OCRP/Projet07/working/data/'\n",
        "df_cleaned_filtered = pd.read_csv(os.path.join(path, 'df_64_features.csv'))\n",
        "print(\"Dimenssion du dataset groupé:\", df_cleaned_filtered.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFfdR3j7Aghv",
        "outputId": "e73ed8c8-00f8-4f9b-fdc0-179849db5783"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimenssion du dataset groupé: (356251, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from contextlib import contextmanager\n",
        "import time\n",
        "\n",
        "@contextmanager\n",
        "def timer(title):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
        "\n",
        "def one_hot_encoder(df, nan_as_category=True):\n",
        "    \"\"\"\n",
        "    Encodage one-hot pour les colonnes catégorielles et booléennes avec get_dummies.\n",
        "\n",
        "    Paramètres :\n",
        "    - df : pandas.DataFrame\n",
        "        Le DataFrame contenant les données.\n",
        "    - nan_as_category : bool, facultatif, default=True\n",
        "        Indique si les NaN doivent être traités comme une catégorie à part.\n",
        "\n",
        "    Retour :\n",
        "    - df : pandas.DataFrame\n",
        "        Le DataFrame avec les colonnes encodées en one-hot.\n",
        "    - new_columns : list\n",
        "        Liste des nouveaux noms de colonnes ajoutées lors de l'encodage.\n",
        "    \"\"\"\n",
        "\n",
        "    original_columns = list(df.columns)\n",
        "\n",
        "    # Identifier les colonnes catégorielles (type object) et booléennes\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    bool_columns = [col for col in df.columns if df[col].dtype == 'bool']\n",
        "\n",
        "    # Combiner les colonnes catégorielles et booléennes pour l'encodage one-hot\n",
        "    columns_to_encode = categorical_columns + bool_columns\n",
        "\n",
        "    # Appliquer l'encodage one-hot\n",
        "    df = pd.get_dummies(df, columns=columns_to_encode, dummy_na=nan_as_category)\n",
        "\n",
        "    # Identifier les nouvelles colonnes créées par l'encodage one-hot\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "\n",
        "    return df, new_columns\n",
        "\n",
        "# Charger le DataFrame avec pandas\n",
        "with timer(\"One-hot encoding\"):\n",
        "    df_encoded, new_cols = one_hot_encoder(df_cleaned_filtered)\n",
        "\n",
        "with timer(\"Nettoyage des noms de colonnes\"):\n",
        "    df_encoded = df_encoded.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '_', x.upper()))\n",
        "\n",
        "with timer(\"Séparation des colonnes cibles et d'identification\"):\n",
        "    features = df_encoded.drop(['TARGET', 'SK_ID_CURR'], axis=1)  # Exclure 'TARGET' et 'SK_ID_CURR'\n",
        "    target = df_encoded['TARGET']  # Colonne cible\n",
        "    ids = df_encoded['SK_ID_CURR']  # Colonne d'identification\n",
        "\n",
        "# Vérifier et supprimer les valeurs manquantes dans la colonne cible\n",
        "with timer(\"Traitement des valeurs manquantes dans la colonne cible\"):\n",
        "    target = target.dropna()\n",
        "    features = features.loc[target.index]\n",
        "    ids = ids.loc[target.index]\n",
        "\n",
        "with timer(\"Imputation des valeurs manquantes avec la médiane\"):\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    features_imputed = pd.DataFrame(imputer.fit_transform(features), columns=features.columns, index=features.index)\n",
        "\n",
        "with timer(\"Réintégration des colonnes cibles et d'identification\"):\n",
        "    df_imputed = pd.concat([features_imputed, target, ids], axis=1)\n",
        "\n",
        "with timer(\"Split stratifié des données\"):\n",
        "    X = df_imputed.drop(['TARGET'], axis=1)  # Features sans la colonne cible\n",
        "    y = df_imputed['TARGET']  # Colonne cible\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Afficher les dimensions des ensembles d'entraînement et de test\n",
        "print(\"Dimensions de l'ensemble d'entraînement (X_train) :\", X_train.shape)\n",
        "print(\"Dimensions de l'ensemble de test (X_test) :\", X_test.shape)\n",
        "print(\"Dimensions de l'ensemble d'entraînement (y_train) :\", y_train.shape)\n",
        "print(\"Dimensions de l'ensemble de test (y_test) :\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhIPDUAhAdkd",
        "outputId": "d84c6eb2-48b5-4176-a902-822571cbace2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoding - done in 0s\n",
            "Nettoyage des noms de colonnes - done in 0s\n",
            "Séparation des colonnes cibles et d'identification - done in 0s\n",
            "Traitement des valeurs manquantes dans la colonne cible - done in 0s\n",
            "Imputation des valeurs manquantes avec la médiane - done in 5s\n",
            "Réintégration des colonnes cibles et d'identification - done in 0s\n",
            "Split stratifié des données - done in 0s\n",
            "Dimensions de l'ensemble d'entraînement (X_train) : (246005, 85)\n",
            "Dimensions de l'ensemble de test (X_test) : (61502, 85)\n",
            "Dimensions de l'ensemble d'entraînement (y_train) : (246005,)\n",
            "Dimensions de l'ensemble de test (y_test) : (61502,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Logistique Regression"
      ],
      "metadata": {
        "id": "mks6ekwGBXbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score, f1_score, log_loss\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Configurer MLFlow\n",
        "mlflow.set_experiment(\"scoring_model_experiment\")\n",
        "\n",
        "\n",
        "# Définir la pipeline avec SMOTE et un modèle de régression logistique\n",
        "model = ImbPipeline([\n",
        "    ('sampling', SMOTE()),\n",
        "    ('classification', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Définir les hyperparamètres à optimiser\n",
        "param_grid = {\n",
        "    'classification__C': [0.1, 1, 10, 100],\n",
        "    'classification__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Configurer GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Logger avec MLFlow\n",
        "with mlflow.start_run():\n",
        "    # Entraîner le modèle avec GridSearchCV\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Obtenir les meilleurs paramètres\n",
        "    best_params = grid_search.best_params_\n",
        "    print(\"Best parameters found: \", best_params)\n",
        "\n",
        "    # Prédictions sur l'ensemble de test\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    y_pred_proba = grid_search.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculer les métriques\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    logloss = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"AUC: {auc}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"Log Loss: {logloss}\")\n",
        "\n",
        "    # Logger les métriques et les paramètres dans MLFlow\n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"AUC\", auc)\n",
        "    mlflow.log_metric(\"Accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"Recall\", recall)\n",
        "    mlflow.log_metric(\"Precision\", precision)\n",
        "    mlflow.log_metric(\"F1 Score\", f1)\n",
        "    mlflow.log_metric(\"Log Loss\", logloss)\n",
        "\n",
        "    # Logger le modèle\n",
        "    mlflow.sklearn.log_model(grid_search.best_estimator_, \"model\")\n",
        "\n",
        "# Afficher les premières lignes des ensembles d'entraînement et de test\n",
        "print(\"Dimensions de l'ensemble d'entraînement (X_train) :\", X_train.shape)\n",
        "print(\"Dimensions de l'ensemble de test (X_test) :\", X_test.shape)\n",
        "print(\"Dimensions de l'ensemble d'entraînement (y_train) :\", y_train.shape)\n",
        "print(\"Dimensions de l'ensemble de test (y_test) :\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "5ispkBFbBOdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWP5njugEsDW",
        "outputId": "c5adbd0a-a116-4638-a8f9-9d6acadaf375"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.15.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==2.15.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.15.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.6)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.26.4)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.1.4)\n",
            "Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (14.0.2)\n",
            "Collecting querystring-parser<2 (from mlflow)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.3.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.31)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Collecting gunicorn<23 (from mlflow)\n",
            "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (5.4.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (2.2.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading databricks_sdk-0.29.0-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (0.4)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting importlib-metadata!=4.7.0,<8,>=3.7.0 (from mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading importlib_metadata-7.2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (24.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (3.20.3)\n",
            "Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (2024.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.15.1->mlflow) (0.5.1)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (2.27.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.15.1->mlflow) (3.19.2)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.15.1->mlflow) (2024.7.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow) (1.16.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (0.6.0)\n",
            "Downloading mlflow-2.15.1-py3-none-any.whl (26.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.29.0-py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.6/505.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: aniso8601, smmap, querystring-parser, Mako, importlib-metadata, gunicorn, graphql-core, deprecated, opentelemetry-api, graphql-relay, gitdb, docker, alembic, opentelemetry-semantic-conventions, graphene, gitpython, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.2.0\n",
            "    Uninstalling importlib_metadata-8.2.0:\n",
            "      Successfully uninstalled importlib_metadata-8.2.0\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 aniso8601-9.0.1 databricks-sdk-0.29.0 deprecated-1.2.14 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 importlib-metadata-7.2.1 mlflow-2.15.1 mlflow-skinny-2.15.1 opentelemetry-api-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 querystring-parser-1.2.4 smmap-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8HSQQuzFXYx",
        "outputId": "993aadf3-ed28-4c31-b8f2-7efb336e13ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mlflow ui"
      ],
      "metadata": {
        "id": "oqQE_Q95FoKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}